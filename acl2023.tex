% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{ACL2023}
\usepackage{mystyle}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{interpretable methods for doc alignment in dialogue}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
TBD
\end{abstract}

\section{Introduction}
In many customer-facing dialogue applications,
customer service interactions must follow safety guidelines.
These guidelines often have a natural sequential order \cite{abcd}:
If a customer is locked out of their account and requests a password reset,
the agent must first verify that the customer is indeed the owner of the account.

All agents, whether human or robot, should follow safety guidelines.
As a result, safety guidelines are often written in natural language.

Our goal is to train dialogue agents that not only follow a set of natural language guidelines,
but explain their actions by pointing to the guidelines.
This allows others to verify whether the guidelines have been followed.

We propose a dialogue model
that explain sequential decisions with guideline-aligned rationales,
and does not require rationale labels.

Experiments show that our model is accurate,
intepretable,
and works at a range of supervision levels.

We present results on three datasets, ranging over a variety of guideline styles.
In ABCD, the guidelines are given to us \citet{abcd}.
In SGD, we write the guidelines ourselves,
using the generative model to aid development.
In doc2dial, we show that our method works for alignment to general document-guided
dialogue as well.

\section{Related work}
The adaptation of large languge models to task-oriented dialogue
has allowed for impressive results in zero-shot generalization,
where models are tested in scenarios that they have not previously seen
\cite{}.
The key idea behind this success is the use of a natural language interface:
specify scenario-specific details using natural language,
and take advantage of the generalization abilities of large language models.

Rule following (SHARC, doc2dial)

Reading manuals for generalization (RTFM, Kartik's work).

\section{Problem setup}
Our goal is to, given a guideline-grounded dialogue $x$
between a customer and agent,
align each turn in $x$ to a sentence $z$ from document $d$ from guidelines $D$.


\section{Method 1: Doc2Dialogue}
We start with a generative model of dialogue that justifies its actions by
aligning to the guidelines as a whole, without sentence alignments.
We call this model Doc2Dialogue.

The model first UNIFORMLY chooses a document in the guideline $d \sim p(d)$,
then generates the dialogue $x \sim p(x \mid d)$.
This yields the joint distribution $p(x,d) = p(x\mid d)p(d)$.

We perform training by optimizing the log marginal likelihood
\begin{equation}
\log\sum_d p(x,d).
\end{equation}

We perform inference online via Bayes' rule:
\begin{equation}
\argmax_d p(d\mid x) = \argmax_d p(x\mid d)p(d),
\end{equation}
which allows us to make predictions for any prefix of $x$.

\textbf{Why not choose document alignments iid at every agent turn?}
We found that using a document to generate only the next agent turn resulted in poor
unsupervised accuracy.
In particular, the document choice model would converge to a uniform distribution.
Additionally, we found that many single turns were well-explained by a large number of different
documents.
The document should instead explain the dialogue as a whole, rather than an individual turn.
It is these two points that led us to consider generating full dialogues given a single document.

\subsection{Approximate inference in training}
We perform additional experiments with an inference network $q(d|x)$ to speed up training.
We approximate the following objective:
\begin{equation}
\label{eqn:fkl}
\log\sum_d p(x,d) - KL[p(d\mid x) || q(d \mid x)].
\end{equation}
This differs from the VAE objective,
$\log p(x) - KL[q(d|x) || p(d|x)]$,
as we use the forward KL.
The forward KL controls the error of importance sampling \cite{chatterjee},
and has been found to work better in variational inference \cite{fkl}.
In particular, the reverse KL is mode-seeking,
allowing the $q(d|x)$ to prefer an incorrect document
even if $p(d|x)$ prefers the correct one.

We make approximations to equation \ref{eqn:fkl} for computational efficiency.
Rather than relying on importance sampling, we use a top-$k$ approximation
of the evidence $p(x)$, posterior $p(z|x)$, and $q(z|x)$:
\begin{align*}
\log\sum_d p(x,d) &\approx \log\sum_{d\in \tilde{D}} p(x,d)\\
KL[p(d\mid x) || q(d \mid x)] &\approx KL[\tilde{p}(d\mid x) || \tilde{q}(d \mid x)]\\
\tilde{p}(d\mid x) &= \frac{p(x,d)}{\sum_{d \in \tilde{D}} p(x,d)}\\
\tilde{q}(d\mid x) &= \frac{q(d\mid x)}{\sum_{d \in \tilde{D}} q(d\mid x)},
\end{align*}
so that the partition functions for $\tilde{p},\tilde{q}$ are approximated over
$\tilde{D} = \text{argtopk } q(d\mid x)$.

We then optimize this objective with respect to the parameters of $p$ and $q$ 
via gradient descent. We treat $\tilde{D}$ as a constant because the topk operation
is not differentiable.

\paragraph{Why not VAE training?} We found VAE training with the usual ELBO
required a baseline and achieved worse accuracy with a leave-one-out baseline.
There are two differences:
First, our objective naturally uses a
\href{https://github.com/justinchiu/elbos-baselines}{regret baseline},
while the leave-one-out baseline can be interpreted as the advantage.
Second, the VAE objective optimizes reverse KL, which has
been shown to work worse than the forward KL \cite{fkl}.

\paragraph{Relation to wake sleep} The wake-phase aims to find
\begin{equation}
\argmax_p \Es{x\sim D}{\Es{z\sim q(z\mid x)}{\frac{\log p(x,z)}{q(z\mid x)}}},
\end{equation}
and the sleep phase finds
\begin{equation}
\argmin_q \Es{x,z\sim p(x,z)}{\log \frac{p(z\mid x)}{q(z\mid x)}},
\end{equation}
the forward KL divergence between the model distribution $p$ and $q$.
In contrast, we optimize a unified objective for both $p$ and $q$,
and do not require samples from the generative model.

\subsection{Parameterization}
We parameterize $p(x|d)$ with a sequence to sequence model such as BART.
The prior $p(d)$ is a uniform distribution.
The inference network $q(d|x) \propto \langle\text{enc}(x),\text{enc}(d)\rangle$
encodes $x,d$ with a transformer such as RoBERTa.


\subsection{Experiments}
We present an experiment on
unsupervised document classification with a generative model of dialogue given document.
We evaluate the document accuracy $d|x$ right before first agent action.
An example of the first agent action is pulling up the customer's account,
at which point the agent should be following a specific document in the guidelines.
The agent should know what the correct document is before taking an action.

We truncate documents and dialogues to 256 tokens.
If the first agent action does not happen in the first 256 tokens of the dialogue,
we evaluate the document prediction at the 256th token.

We provide a fully supervised skyline as reference, which directly models
$p_s(d|x) \propto \langle \text{emb}(d), \text{RoBERTa}(x)\rangle$,
where we use a label embedding of the document $d$ that does not see
the document's text (a RoBERTa encoding of the document performed similarly).
As a baseline, we use BM25 which is based on lexical similarity.

\begin{table*}
\centering
\begin{tabular}{llrrr}
\toprule
model                             & Training objective     & $N$   & doc acc & training time\\
\midrule
Skyline supervised                & $\log p_s(d|x)$        & 8,034 &    90.7 & -\\
Baseline lexical                  & -                      & 0     &    34.1 & -\\
Doc2Dialogue                      & $\log \sum_dp(x,d)$    & 0     &    71.8 & 20h\\
Doc2Dialogue                      & $\log \sum_{d\in\tilde{D}} p(x,d) - KL[\tilde{p}||\tilde{q}]$ & 0     &    75.8 & 14h\\
\bottomrule
\end{tabular}
\caption{
\label{tbl:unsup-doc}
Results for document classification with a generative model at the first
agent action in a conversation. Documents and dialogues are truncated to the first 256
tokens.
The number of labeled training examples is $N$.}
\end{table*}

In table \ref{tbl:unsup-doc}, we see that both full marginalization and approximate inference
outperform the lexical baseline, but do not reach the supervised skyline.
One cause of this is that a portion of the documents in the guidelines are lexically identical to one
another, resulting in a performance drop for the unsupervised models.
The supervised skyline does not use the documents themselves and therefore does not
suffer from this issue.
Additionally, the unsupervised model has issues distinguishing between similar
documents, such as changing the name of an account versus changing the address.

Doc2Dialogue with full marginalization has a worse document accuracy than approximate inference
because of model selection.
We report accuracy after 10 epochs of training, since we do not assume access to
examples with labeled document alignments.
Full marginalization reaches a similar document accuracy in the middle of training,
but the accuracy subsequently drops as training continues.

\section{Method 2: Sent2Dialogue}
The next step is to model sentence alignments given the oracle document.
We start with a very simple alignment model, which we call Sent2Dialogue.

We break up the dialogue into turns $x = (x_1,\ldots,x_T)$.
For every turn, we first choose a sentence $z_t$ from a uniform $p(z_t|d)$,
then generate the utterance for that turn $p(x_t|x_{<t}, z_t)$ with BART
(the encoder input tokens are given by $z_t$ and decoder input tokens by $x_{<t}$).
The graphical model is given in figure \ref{fig:pgm-sent1}.

We directly optimize the marginal likelihood
\begin{equation}
\begin{aligned}
&\log p(x_{1:T}\mid d)\\
&= \log \prod_t \sum_{z_t} p(x_t \mid x_{<t}, z_t) p(z_t\mid d).
\end{aligned}
\end{equation}
\footnote{A natural extension of this model is to instead model $p(z_t|x_{<t},d)$,
we can approximate the marginal likelihood by taking the top-k elements of this
more informative prior.}

For each turn, we perform inference via Bayes' rule
\begin{equation}
\begin{aligned}
&\argmax_{z_t} p(z_t \mid x, d)\\
&= \argmax_{z_t} p(x_t \mid x_{<t}, z_t)p(z_t\mid d),
\end{aligned}
\end{equation}
where $p(z_t|d)$ is uniform and can be dropped.

\begin{figure}[t]
\begin{center}
\scalebox{.85}{
\begin{tikzpicture}

\node[obs] (d) {$d$};
\node[latent, below=of d] (z1) {$z_1$};
\node[latent, right=of z1] (z2) {$z_2$};
\node[latent, right=of z2] (z3) {$z_3$};
\node[obs, below=of z1] (x1) {$x_1$};
\node[obs, below=of z2] (x2) {$x_2$};
\node[obs, below=of z3] (x3) {$x_3$};

\edge[->] {d} {z1};
\edge[->] {d} {z2};
\edge[->] {d} {z3};

%\edge[->] {z1} {z2};
%\edge[->] {z2} {z3};

\edge[->] {z1} {x1};
\edge[->] {z2} {x2};
\edge[->] {z3} {x3};

\edge[->] {x1} {x2};
\draw[->] (x1) to[bend right=30] (x3);
\edge[->] {x2} {x3};

\end{tikzpicture}
}
\end{center}
\caption{Graphical model for the first approach to sentence alignment.
}
\label{fig:pgm-sent1}
\end{figure}

\subsection{Experiments}
We evaluate unsupervised sentence alignment models,
and report the average validation sentence accuracy across all turns.
We annotate the validation sentence alignments ourselves.

We do not truncate documents or dialogues in this experiment.

We do not have access to training labels, and therefore do not have a supervised skyline.
We use as a baseline BM25, which measures lexical similarity.

We present results in table \ref{tbl:unsup-sent} TBD.

\begin{table*}
\centering
\begin{tabular}{llrrr}
\toprule
model                             & Training objective     & $N$   & sent acc & training time\\
\midrule
Baseline lexical                  & -                      & 0     &    & -\\
Sent2Turn                         & $\log p(x|d)$          & 0     &    & -\\
\bottomrule
\end{tabular}
\caption{
\label{tbl:unsup-sent}
Results for document classification with a generative model at the first
agent action in a conversation. Documents and dialogues are truncated to the first 256
tokens.
The number of labeled training examples is $N$.}
\end{table*}

\begin{figure}[t]
\begin{center}
\scalebox{.85}{
\begin{tikzpicture}

\node[latent] (d) {$d$};
\node[latent, below=of d] (z1) {$z_1$};
\node[latent, right=of z1] (z2) {$z_2$};
\node[latent, right=of z2] (z3) {$z_3$};
\node[obs, below=of z1] (x1) {$x_1$};
\node[obs, below=of z2] (x2) {$x_2$};
\node[obs, below=of z3] (x3) {$x_3$};

\edge[->] {d} {z1};
\edge[->] {d} {z2};
\edge[->] {d} {z3};

\edge[->] {z1} {z2};
\edge[->] {z2} {z3};

\edge[->] {z1} {x1};
\edge[->] {z2} {x2};
\edge[->] {z3} {x3};

\edge[->] {x1} {x2};
\draw[->] (x1) to[bend right=30] (x3);
\edge[->] {x2} {x3};

\end{tikzpicture}
}
\end{center}
\caption{Graphical model for full document and span alignments.
}
\label{fig:pgm-doc-sent}
\end{figure}



\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Experiment 1: Document classification from observed dialogue + action output}
\subsection{Research question}
Can we do document classification in document-driven dialogue with no document labels?

\subsection{Experiment}
We present experiments on
unsupervised document classification with a generative model of dialogue given document.
We evaluate the document accuracy $d|x$ right before first agent action.
An example of the first agent action is pulling up the customer's account,
at which point the agent should be following a specific document in the guidelines.
The agent should know what the correct document is before taking an action.

\subsection{Models}
\begin{itemize}
\item Skyline: supervised $$p(d \mid x) \propto \langle \text{emb}(d_{label}), \text{BERT}(x)\rangle$$
\item Baseline: lexical BM25
\item Approximate marginalization with $D^*$: $\log\sum_{d\in D^*} p(x\mid d) p(d)$
    \begin{itemize}
    \item $D^*$ = \{true $d^*$, 3 hard lexical negatives based on $d^*$, 3 random negatives\}
    \item Inference via Bayes' rule: $\argmax_d p(x_{1:t}|d)$ where $t$ is the index
        of the first agent action.
    \end{itemize}
\item Full marginalization over D: $\log\sum_{d\in D} p(x|d)p(d)$
    \begin{itemize}
    \item all docs $D$
    \item Inference via Bayes' rule
    \end{itemize}
\item Approximate marginalization with $q(d\mid x)$:
    $\log\sum_{d\in \tilde{D}} p(x|d)p(d) - KL[\tilde{p}(d\mid x)||\tilde{q}(d\mid x)]$
    \begin{itemize}
    \item $\tilde{D}$ from top16 $q(d|x)$
    \item Inference via Bayes'
    \end{itemize}
\end{itemize}

\subsection{Results}

\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
model & acc & time\\
\midrule
Skyline supervised $p(d|x)$ & 90.7 & -\\
Baseline lexical & 34.1 & -\\
Approx marg w/ $D^*$ & 78 & 6h\\
Full marg w/ $D$ & 71.8 & 20h\\
Approx marg w/ top16 $q(d\mid x)$ & 75.8 & 14h\\
\bottomrule
\end{tabular}
\caption{
\label{tbl:unsup-doc-app}
Results for document classification with a generative model at the first
agent action in a conversation.}
\end{table}

In table \ref{tbl:unsup-doc-app}, we see that
\begin{itemize}
\item Full marg does better than lexical baseline, but worse than approximate marg over Z*.
\item Approximate marg with $q$ does better than full marg, but worse than approximate marg over Z*.
\end{itemize}
The approximate marg w/ Z* doing best is surprising,
since the training setup of full marg (all $D$) is closer to the testing setup (all $D$),
as $D^*\subset D$.

There are a few possible causes
\begin{enumerate}
\item There are reasonable negative documents in $D \setminus D^*$ that have $p(x|d) > p(x|d^*)$
\item The gradient will be larger for $D^*$ since it is smaller, meaning the support will be smaller
    and the posterior will be sharper (gradient = posterior).
\end{enumerate}
Error analysis found the the model does make mistakes in full marg and approx marg with $q$
due to the existence of distracting negatives not considered in $D^*$.
However, there are also other errors in approx marg with $q$ that are not explained by this.

\subsection{Model selection}
Since we do not assume access to labeled examples, we perform model selection using the
validation likelihood $p(x)$.
The validation likelihood always decreases over the course of training,
meaning we simply take the last model checkpoint after 10 epochs.
This results in a decent amount of overfitting, meaning the presented results are not the
highest accuracies over the course of training.

\section{Experiment 2: Semi-supervised document classificaiton}
\subsection{Question}
When writing a manual, we need at least one labeled example for each document in the manual.
How many labeled examples do we need to recover supervised performance?

\subsection{Experiment}
We perform early document detection in the semi-supervised setting,
where we have 50 paired $(x,d)$ examples, and the remaining examples only have access to $x$.
We report accuracy on $d|x$ at the first agent action.

\subsection{Models}
\begin{itemize}
\item Skyline: fully supervised $p(d|x)$
\item Baseline: Approx marg w/ $q(d|x)$ with no $d$ labels
\item Approx marg w/ top16 $q(d|x)$ with $\set{10,50,100}$ $d$ labels
\end{itemize}

Ways of utilizing labeled examples:
\begin{itemize}
\item Warm start: Train $p$ and $q$ on these labeled examples
\item Interleave: Every $M$ steps, train $p$ and $q$ on these examples
\end{itemize}
In order to prevent overfitting to the labeled examples, we use interleaving
(find citation for KL reg / semisup).

\subsection{Results}
In table \ref{tbl:semisup-doc}, we see that TBD

\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
model & $N$ & doc acc\\
\midrule
Skyline supervised $p(d|x)$ & All & 90.65\\
Approx marg w/ top16 $q(d|x)$ & 0 & 74.2\\
Approx marg w/ top16 $q(d|x)$ & 10 & -\\
Approx marg w/ top16 $q(d|x)$ & 50 & -\\
Approx marg w/ top16 $q(d|x)$ & 100 & -\\
\bottomrule
\end{tabular}
\caption{
\label{tbl:semisup-doc}
Results for document classification with a generative model.
$N$ is the number of labeled examples seen during training.}
\end{table}

\section{Experiment 3: Span alignment}
\subsection{Question}
When writing a manual, we need at least one labeled example for each document in the manual.
How many labeled examples do we need to recover supervised performance?


\subsection{Results}
\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
model & $N$ & span acc\\
\midrule
Baseline BM25 & 0 & -\\
Approx marg w/ top16 $q(a|x,d)$ & 0 & -\\
Approx marg w/ top16 $q(a|x,d)$ & 10 & -\\
Approx marg w/ top16 $q(a|x,d)$ & 50 & -\\
\bottomrule
\end{tabular}
\caption{
\label{tbl:unsup-span}
Results for document classification with a generative model.
$N$ is the number of labeled examples seen during training.}
\end{table}

\section{Experiment 4: Downstream AST accuracy}

\section{Experiment 5: Downstream response generation}

\section{Experimental setup}

\section{Results}\section{Sasha questions about doc classification (12/27)}
\begin{itemize}
\item Isn't your model p(x, z)
    \begin{itemize}
    \item Yes, the model is $p(x\mid z)p(z)$, with $p(z)$ uniform.
    \end{itemize}
\item I don't really like this experiment, because it seems to test two different things:
    1) keeping the $z^*$ in the true set, 2) approximating the marginalization.
    A clean experiment would be Full Marginalization vs. Approx Marginalization during training.
    The one that keeps around $Z^*$ is a skyline at best, and maybe at worst not informative.
    \begin{itemize}
    \item The approximate marginalization with $Z^*$ will not be included in the final results,
        but was useful for debugging full marg and will be useful for debugging the VAE setting.
    \item That said, this is a clean experiment. Only one thing is changed: the set of negatives.
        Approx marg w/ $Z^*$ uses $z^*$ and some negatives,
        while full marg uses $z^*$ and all negatives.
        Full marg vs VAE approx marg would change both the negatives as well as whether $z^*$
            is guaranteed to be present.
    \end{itemize}
\item I would like your conclusions to be a little bit more clear about things like
    speed and methods. Is Full Marg reasonable or not?
    \begin{itemize}
    \item Speed: Full marg takes between 5-10 hours to reach peak validation document accuracy
        This is reasonable for this setting, but will become a limitation in models
        that must perform both sentence and document marginalization.
    \item General resaonableness: Full marg is reasonable as long as it fits within memory constraints.
        It is reasonable for this dataset, but may not be for the other datasets.
    \end{itemize}
\item The name "Approx Marg" does not really make sense here, as again approx would be a version of this with the $Z*$
    \begin{itemize}
    \item Approximate marginalization with $Z^*$ describes the setting $\log\sum_{z\in Z^*} p(x,z), Z^*\subset Z$.
        Marginalization over $Z$ is approximated over the restriction $Z^*$.
        I believe this is a precise description without jargon.
    \end{itemize}
\item You are much too early to worry about hyperparams, that discussion should not even be here yet.
    \begin{itemize}
    \item I managed to get accuracy up a few points, but nothing major. Other learning rate
        settings resulted in very poor performance for this experiment,
        as fine-tuning is sensitive to hyperparameters.
    \end{itemize}
\item I don't really get this line "This is surprising, since the training setup (all Z) is closer to the testing setup (all Z), as Z* is a strict subset of Z". This doesn't seem surprising to me?
    \begin{itemize}
    \item It is hard to predict whether approximate marginalization with $Z^*$
        vs full marginalization with Z would yield a better model.
    \item $p(x|z)$ will learn to prefer $z^*$ if $p(x|z^*)$ is better than other $p(x|z)$,
        since the gradient of the log marginal likelihood objective is the posterior $p(z|x)$
        and the model has a uniform $p(z)$.
    \item When would approx marg w/ Z* do better?
        If $Z^*$ contains hard negatives $z$ with $p(x | z^*) > p(x | z)$ but not negatives $p(x | z^*) < p(x | z)$,
        so that the model doesnt learn to prefer those hard negatives over the true $z^*$.
        This seems to be the case here.
    \item When would approx marg w/ $Z^*$ do worse?
        If $Z^*$ misses some hard negatives with $p(x | z^*) > p(x | z)$.
    \end{itemize}
\item please provide a section in these documents with "parameterization". Is $p(z)$ parameterized?
    \begin{itemize}
    \item $p(z)$ is uniform and therefore has no learnable parameters.
    \end{itemize}
\end{itemize}

\end{document}
