% This must be in the first 5 lines to tell arXiv to use pdfLaTeX, which is strongly recommended.
\pdfoutput=1
% In particular, the hyperref package requires pdfLaTeX in order to break URLs across lines.

\documentclass[11pt]{article}

% Remove the "review" option to generate the final version.
\usepackage[review]{ACL2023}
\usepackage{mystyle}

% Standard package includes
\usepackage{times}
\usepackage{latexsym}

% For proper rendering and hyphenation of words containing Latin characters (including in bib files)
\usepackage[T1]{fontenc}
% For Vietnamese characters
% \usepackage[T5]{fontenc}
% See https://www.latex-project.org/help/documentation/encguide.pdf for other character sets

% This assumes your files are encoded as UTF8
\usepackage[utf8]{inputenc}

% This is not strictly necessary, and may be commented out.
% However, it will improve the layout of the manuscript,
% and will typically save some space.
\usepackage{microtype}

% This is also not strictly necessary, and may be commented out.
% However, it will improve the aesthetics of text in
% the typewriter font.
\usepackage{inconsolata}


% If the title and author information does not fit in the area allocated, uncomment the following
%
%\setlength\titlebox{<dim>}
%
% and set <dim> to something 5cm or larger.

\title{interpretable methods for doc alignment in dialogue}

% Author information can be set in various styles:
% For several authors from the same institution:
% \author{Author 1 \and ... \and Author n \\
%         Address line \\ ... \\ Address line}
% if the names do not fit well on one line use
%         Author 1 \\ {\bf Author 2} \\ ... \\ {\bf Author n} \\
% For authors from different institutions:
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \And  ... \And
%         Author n \\ Address line \\ ... \\ Address line}
% To start a seperate ``row'' of authors use \AND, as in
% \author{Author 1 \\ Address line \\  ... \\ Address line
%         \AND
%         Author 2 \\ Address line \\ ... \\ Address line \And
%         Author 3 \\ Address line \\ ... \\ Address line}

\author{First Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\\And
  Second Author \\
  Affiliation / Address line 1 \\
  Affiliation / Address line 2 \\
  Affiliation / Address line 3 \\
  \texttt{email@domain} \\}

\begin{document}
\maketitle
\begin{abstract}
TBD
\end{abstract}

\section{Introduction}
In many customer-facing dialogue applications,
customer service interactions must follow a set of guidelines for safety,
which have a natural sequential order.
If a customer is locked out of their account and requests a password reset,
the agent must first verify that the customer is indeed the owner of the account.
This if-then structure is common in guidelines \cite{abcd}.

All agents, whether human or robot, must follow safety guidelines.
As a result, safety guidelines are often written in natural language.
Natural language guidelines also allow for zero-shot generalization to scenarios
that may be new to an agent, but described similarly in the guidelines to
familiar scenarios.

Our goal is to train dialogue agents that not only follow a set of guidelines,
but justify their actions by pointing to the guidelines.
This allows others to verify their actions, and whether the guidelines have been followed.

We propose a generative model of dialogue,
that justifies decisions by aligning to a guidelines,
utilizes the sequential structure of guidelines,
and does not require supervision.

Experiments show that our model is accurate,
intepretable,
and works at a range of supervision levels.

We present results on three datasets, ranging over a variety of guideline styles.
In ABCD, the guidelines are given to us \citet{abcd}.
In SGD, we write the guidelines ourselves,
using the generative model to aid development.
In doc2dial, we show that our method works for alignment to general document-guided
dialogue as well.

\section{Related work}
The adaptation of large languge models to task-oriented dialogue
has allowed for impressive results in zero-shot generalization,
where models are tested in scenarios that they have not previously seen
\cite{}.
The key idea behind this success is the use of a natural language interface:
specify scenario-specific details using natural language,
and take advantage of the generalization abilities of large language models.


\section{Problem setup}
Our goal is to, given an observed task-oriented and guideline-grounded dialogue $x$
between a customer and agent,
justify the actions of the agent by aligning them to natural language guidelines $z$.

\section{Method}
We propose a generative model of dialogue that justifies its actions by
aligning to the guidelines.

The model first chooses a document in the guideline $z \sim p(z)$,
then generates the dialogue $x \sim p(x \mid z)$.
This yields the joint distribution $p(x,z) = p(x\mid z)p(z)$.

We perform training by optimizing the log marginal likelihood
\begin{equation}
\log\sum_z p(x,z).
\end{equation}

We perform inference online via Bayes' rule:
\begin{equation}
\argmax_z p(z\mid x) = \argmax_zp(x\mid z)p(z).
\end{equation}

\textbf{Why not break down alignments at the turn-level?}
We found that using a document to generate only the next agent turn resulted in poor
unsupervised accuracy (degeneration to a uniform distribution).
Additionally, we found that many single turns were well-explained by a large number of different
documents.
It is these two points that led us to consider generating full dialogues given a single document,
so that document must explain multiple turns at once.
This is because documents in guidelines share many common actions,
and it is the sequencing of these actions that distinguishes them.
Therefore modeling the whole dialogue allows the model to take into account full
sequences of actions.
Note that this is only for documents. We will perform lower-level alignments at
the turn-level, while keeping document selection at the full dialogue level.

\subsection{VAE and Wake-sleep training}
We perform additional experiments with an inference network to speed up training
over full marginalization.
We propose two objectives:
\begin{align}
&\log\sum_z p(x,z) - KL[q(z\mid x) || p(z \mid x)]\\
&\log\sum_z p(x,z) - KL[p(z\mid x) || q(z \mid x)].
\end{align}
The first we will refer to as VAE, and the second as wake-sleep.
We optimize the first in the standard VAE setting by optimizing the usual variational evidence
lower bound with a baseline for the gradient estimator.
For wake-sleep, we make the following approximations:
approximate the reconstruction term
\begin{align*}
\log\sum_z p(x,z) &\approx \log\sum_{z\in Z} p(x,z)\\
KL[p(z\mid x) || q(z \mid x)] &\approx KL[\tilde{p}(z\mid x) || \tilde{q}(z \mid x)]\\
\tilde{p}(z\mid x) &= \frac{p(x,z)}{\sum_{z \in Z} p(x,z)}\\
\tilde{q}(z\mid x) &= \frac{q(z\mid x)}{\sum_{z \in Z} q(z\mid x)},
\end{align*}
so that $\tilde{p},\tilde{q}$ are only normalized over $Z = \text{argtopk} q(z\mid x)$.

\section{Span alignments}
Assert no multi-hop.

\begin{figure}[t]
\begin{center}
\scalebox{.85}{
\begin{tikzpicture}

\node[latent] (z) {$z$};
\node[latent, below=of z] (a1) {$a_1$};
\node[latent, right=of a1] (a2) {$a_2$};
\node[latent, right=of a2] (a3) {$a_3$};
\node[obs, below=of a1] (x1) {$x_1$};
\node[obs, below=of a2] (x2) {$x_2$};
\node[obs, below=of a3] (x3) {$x_3$};

\edge[->] {z} {a1};
\edge[->] {z} {a2};
\edge[->] {z} {a3};

\edge[->] {a1} {a2};
\edge[->] {a2} {a3};

\edge[->] {a1} {x1};
\edge[->] {a2} {x2};
\edge[->] {a3} {x3};

\edge[->] {x1} {x2};
\draw[->] (x1) to[bend right=30] (x3);
\edge[->] {x2} {x3};

\end{tikzpicture}
}
\end{center}
\caption{Graphical model for full document and span alignments.
}
\label{fig:pgm-sent}
\end{figure}

\section{Experimental setup}

\section{Results}

\begin{table}
\centering
\begin{tabular}{lrr}
\toprule
model & $N$ & doc acc\\
\midrule
Skyline supervised $p(z|x)$ & All & 90.65\\
Baseline lexical & 0 & 34.06\\
$p(x,z)$ & 0 & 74.2\\
$p(x,z)$ & 50 & -\\
$p(x,z)$ & 100 & -\\
\bottomrule
\end{tabular}
\caption{
\label{tbl:unsup-doc}
Results for document classification with a generative model.
$N$ is the number of labeled examples seen during training.}
\end{table}


\bibliography{anthology,custom}
\bibliographystyle{acl_natbib}

\appendix

\section{Report and questions 12/27}
\subsection{Research question}
Can we do document classification in document-driven dialogue with as few document labels as possible?

\subsection{Experiment}
Unsupervised document classification with a generative model of dialogue given document.
document accuracy $z|x$ right before first agent action.
The first agent action will be something like pulling up the customer's account,
at which point the agent should be following a document.
The agent will always know what the correct document is before taking an action,
but they will definitely know what the correct document is right before they take an action.

\subsection{Models}
\begin{itemize}
\item Skyline: supervised $$p(z \mid x) \propto \langle emb(z_{label}), BERT(x)\rangle$$
\item Baseline: lexical BM25
\item Approximate marginalization with $Z^*$: $\log\sum_{z\in Z*} p(x\mid z) p(z)$
    \begin{itemize}
    \item $Z^*$ = \{true $z^*$, 3 hard lexical negatives based on $z^*$, 3 random negatives\}
    \item uniform $p(z)$
    \item BART $p(x \mid z)$
    \item Inference via Bayes' rule: $\argmax_z p(x_{1:t}|z)$ where $t$ is the index
        of the first agent action.
    \end{itemize}
\item Full marginalization over Z: $log\sum_{z\in Z} p(x|z)p(z)$
    \begin{itemize}
    \item all docs $Z$
    \item uniform $p(z)$
    \item BART $p(x \mid z)$
    \item Inference via Bayes' rule: $\argmax_z p(x_{1:t}|z)$ where $t$ is the index
        of the first agent action.
    \end{itemize}
\end{itemize}

\subsection{Results}

\begin{table}
\centering
\begin{tabular}{lr}
\toprule
model & acc\\
\midrule
Skyline supervised $p(z|x)$ & 90.65\\
Baseline lexical & 34.06\\
Approx marg w/ $Z^*$ & 80.18\\
Full marg w/ $Z$ & 74.2\\
\bottomrule
\end{tabular}
\caption{
\label{tbl:unsup-doc-app}
Results for document classification with a generative model at the first
agent action in a conversation.}
\end{table}

See table \ref{tbl:unsup-doc-app}.

\begin{itemize}
\item Full marg does better than lexical baseline
\item Full marg does worse than approximate marg over Z*
\end{itemize}
This is surprising, since the training setup (all $Z$) is closer to the testing setup (all $Z$),
as $Z^*\subset Z$.

Two possible causes
\begin{enumerate}
\item Different hyperparameters: I had to use no batching for full marg, but didn’t sweep over hyperparams. Learning rate should scale with batch size (citation: https://arxiv.org/abs/1706.02677)
\item There are reasonable negative documents in $Z \setminus Z^*$ that have $p(x|z) > p(x|z^*)$
\end{enumerate}

\subsection{Immediate next steps}
\begin{itemize}
\item Error analysis, comparing the things full marg got wrong but approx marg got right.
\item Are there negatives that were excluded by Z*, that end up making performance worse?
Hyperparam sweep for full marg.
\item Speed up full marg with an inference network q(z|x) in the VAE setting.
There is only enough memory on the A100s to run full marg unbatched. This slows down iteration speed and will make further modeling difficult.
\end{itemize}

\subsection{Sasha questions}
\begin{itemize}
\item Isn't your model p(x, z)
    \begin{itemize}
    \item Yes, the model is $p(x\mid z)p(z)$, with $p(z)$ uniform.
    \end{itemize}
\item I don't really like this experiment, because it seems to test two different things:
    1) keeping the $z^*$ in the true set, 2) approximating the marginalization.
    A clean experiment would be Full Marginalization vs. Approx Marginalization during training.
    The one that keeps around $Z^*$ is a skyline at best, and maybe at worst not informative.
    \begin{itemize}
    \item The approximate marginalization with $Z^*$ will not be included in the final results,
        but was useful for debugging full marg and will be useful for debugging the VAE setting.
    \item That said, this is a clean experiment. Only one thing is changed: the set of negatives.
        Approx marg w/ $Z^*$ uses $z^*$ and some negatives,
        while full marg uses $z^*$ and all negatives.
        Full marg vs VAE approx marg would change both the negatives as well as whether $z^*$
            is guaranteed to be present.
    \end{itemize}
\item I would like your conclusions to be a little bit more clear about things like
    speed and methods. Is Full Marg reasonable or not?
    \begin{itemize}
    \item Speed: Full marg takes between 5-10 hours to reach peak validation document accuracy
        This is reasonable for this setting, but will become a limitation in models
        that must perform both sentence and document marginalization.
    \item General resaonableness: Full marg is reasonable as long as it fits within memory constraints.
        It is reasonable for this dataset, but may not be for the other datasets.
    \end{itemize}
\item The name "Approx Marg" does not really make sense here, as again approx would be a version of this with the $Z*$
    \begin{itemize}
    \item Approximate marginalization with $Z^*$ describes the setting $\log\sum_{z\in Z^*} p(x,z), Z^*\subset Z$.
        Marginalization over $Z$ is approximated over the restriction $Z^*$.
        I believe this is a precise description without jargon.
    \end{itemize}
\item You are much too early to worry about hyperparams, that discussion should not even be here yet.
    \begin{itemize}
    \item I managed to get accuracy up a few points, but nothing major. Other learning rate
        settings resulted in very poor performance for this experiment,
        as fine-tuning is sensitive to hyperparameters.
    \end{itemize}
\item I don't really get this line "This is surprising, since the training setup (all Z) is closer to the testing setup (all Z), as Z* is a strict subset of Z". This doesn't seem surprising to me?
    \begin{itemize}
    \item It is hard to predict whether approximate marginalization with $Z^*$
        vs full marginalization with Z would yield a better model.
    \item $p(x|z)$ will learn to prefer $z^*$ if $p(x|z^*)$ is better than other $p(x|z)$,
        since the gradient of the log marginal likelihood objective is the posterior $p(z|x)$
        and the model has a uniform $p(z)$.
    \item When would approx marg w/ Z* do better?
        If $Z^*$ contains hard negatives $z$ with $p(x | z^*) > p(x | z)$ but not negatives $p(x | z^*) < p(x | z)$,
        so that the model doesnt learn to prefer those hard negatives over the true $z^*$.
        This seems to be the case here.
    \item When would approx marg w/ $Z^*$ do worse?
        If $Z^*$ misses some hard negatives with $p(x | z^*) > p(x | z)$.
    \end{itemize}
\item please provide a section in these documents with "parameterization". Is $p(z)$ parameterized?
    \begin{itemize}
    \item $p(z)$ is uniform and therefore has no learnable parameters.
    \end{itemize}
\end{itemize}

\end{document}
